{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514b6ec-a794-43a3-a241-1b64dee6145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dab6f-f396-4acb-b3eb-b0ec04f5e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reddit posts from subreddit WallStreetBets, downloaded from \n",
    "#https://www.reddit.com/r/wallstreetbets/ using praw (The Python Reddit API Wrapper).\n",
    "wsb = pd.read_csv('./reddit_wsb4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc069a1b-1743-4a78-813c-9c7d8f18cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamestop minute stock price data was downloaded from alphavantage.com\n",
    "#(You will need to sign up for free to get an api-key)\n",
    "#from alpha_vantage.timeseries import TimeSeries\n",
    "#df = pd.read_csv('https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=GME&interval=1min&slice=year1month1&apikey=#####')\n",
    "#print(df)\n",
    "#df.to_csv('gme1min4.csv')\n",
    "gme = pd.read_csv('./gme_clean_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66569c-b164-497d-91f3-6555d4351d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted the timestamps to datetime in order to merge the two dataframes\n",
    "wsb['time'] = pd.to_datetime(wsb['time'])\n",
    "gme['time'] = pd.to_datetime(gme['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbb0ed-fb63-439a-84dd-787874a9540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged my data using the time column, so I now every post had stock data\n",
    "subset =wsb.merge(gme, how='left', on=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd267ac-602a-47da-986e-496c78a51726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved my dataset as a CSV file\n",
    "subset.to_csv('cleanWsbGME.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937afee-e163-496b-91a0-b8a44df1d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decided to only use the title, dropped unneccesary columns \n",
    "f_data = pd.read_csv('./wsbGMEFilled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059855-066d-4b2d-8317-4ac25b385c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda063d6-2ce7-4980-a9c8-a8beeaff5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc1491-b595-49c2-941c-c97ef6006446",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data.title =f_data.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d284a0e-e523-4cea-b604-e7957cbfb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the special characters\n",
    "f_data.title = f_data.title.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "# Remove URLS\n",
    "f_data.title = f_data.title.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74361cb5-d9da-4c37-a521-f99f943c5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While APIs like demoji will easily convert emojis into text, I did not want to lose\n",
    "#context and instead got the intended meanings from WallstreetBets lingo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83651e50-17bf-459d-95ef-d80c32549b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data['title'] = f_data['title'].str.replace('ğŸ’',' strong ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒˆ',' non believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('âœˆï¸',' going up ')\n",
    "f_data['title'] = f_data['title'].str.replace('âœˆ',' going up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¦',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸš€',' going up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒ™',' going up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ§»',' weak ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ—',' money ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ‚',' non believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ–',' believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ‘',' believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ’',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ™‚',' slight smile ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ’©',' shit ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜‚',' laughing ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ™Œ',' believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒ',' going up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¥œ',' senseless ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒ•',' up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒ“',' up ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¥³',' party ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¥°',' love ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ‘Œ',' good ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ™ŒğŸ¾',' believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ»',' non believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('â¤ï¸',' love ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜¢',' crying ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¦§',' believer ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜',' cool ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ’ª',' strong ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤—',' hugs ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ’',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¸ğŸ¦',' doubt ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤¡',' clown ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤š',' hold ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ”¥',' active ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ–',' suckers ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ™',' together ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸµ',' singing ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜¾',' disapproval ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸŒ',' slight smile ')\n",
    "f_data['title'] = f_data['title'].str.replace('âœŠ',' believe ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤²',' pray ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤©',' love ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜',' sad ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜˜',' love ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜¡',' upset ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜ ',' upset ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜¤',' angry ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸµ',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤£',' laughing ')\n",
    "f_data['title'] = f_data['title'].str.replace('âœ‹ğŸ½',' do not sell ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ‘',' clapping ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤',' hopefull ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜ˆ',' trouble ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¤”',' thinking ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ‘‹',' hello ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ¥¶',' grimace ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜­',' crying ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ˜Š',' smile ')\n",
    "f_data['title'] = f_data['title'].str.replace('ğŸ’°',' money ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e1bdb-161f-43df-b934-1005dfe69231",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data['title'] = f_data['title'].str.replace('fomo',' fear of missing out ')\n",
    "f_data['title'] = f_data['title'].str.replace('ape',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('yolo',' risk ')\n",
    "f_data['title'] = f_data['title'].str.replace('autist',' intelligent friends')\n",
    "f_data['title'] = f_data['title'].str.replace('moon',' up ')\n",
    "f_data['title'] = f_data['title'].str.replace('retard',' friend ')\n",
    "f_data['title'] = f_data['title'].str.replace('tendie',' money ')\n",
    "f_data['title'] = f_data['title'].str.replace('stonk',' stock ')\n",
    "f_data['title'] = f_data['title'].str.replace('loss porn',' lose money ')\n",
    "f_data['title'] = f_data['title'].str.replace('gain porn',' make money ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def482d5-925d-439c-ab92-97cd0c5cb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data['title'] = f_data['title'].str.replace('\\d+','', regex=True)\n",
    "f_data[\"title\"] = f_data[\"title\"].str.replace(\"''\",\" \", regex=True)\n",
    "#remove duplicate words\n",
    "f_data['title'] = f_data['title'].str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1',regex=True)\n",
    "f_data['title'] = f_data['title'].str.replace('[<,?,=,>,$,@,!,%,^,&,*,/,\\,+,-,~,;,-,.,:]','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44346b6f-ccd5-40b8-9ed9-66a52ac015e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the extra spaces\n",
    "f_data['title'] = f_data['title'].str.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985245d-43bc-4712-ac79-0418d94a544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are more efficient ways of saving work in progress this but I worked on this on \n",
    "#multiple days and computers \n",
    "f_data.to_csv('wsbfinal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db8a9b-dd4f-4f2c-85b3-dd09f6317d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d32a0a-d057-488d-96f8-365525860dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36e772-c0e9-4aa6-afad-8e41c783abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f5b7e-55ba-4e50-a474-f8dec356afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import text2emotion as te\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from raceplotly.plots import barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83935e3e-34bb-4cba-b1bb-b242a4fad0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = pd.read_csv('./wsbTitleFilled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75166843-a12b-4da9-b624-a329c5c4ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521aade-043b-4b38-863b-3111fd7eafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to Puru Behl for this code \n",
    "#https://www.kaggle.com/accountstatus/wall-streets-bet-data-analysis/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bba29b-f4f7-48ba-9a8b-a4395318f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the emotions to the data and Date features to the data\n",
    "# Use this code if you wanna make the sentiments from scratch\n",
    "t=[]\n",
    "count=0\n",
    "for i in wsb['title'].values:\n",
    "    count+=1\n",
    "    print(count*100/len(wsb))\n",
    "    clear_output(wait=True)\n",
    "    t.append(te.get_emotion(i))\n",
    "t=np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048f19b-bb6d-4aa7-a5c7-6c0e8a150977",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb['overall text']=wsb.title.astype(\"str\")\n",
    "wsb['Happy']=[dict(i)['Happy'] for i in t]\n",
    "wsb['Angry']=[dict(i)['Angry'] for i in t]\n",
    "wsb['Surprise']=[dict(i)['Surprise'] for i in t]\n",
    "wsb['Sad']=[dict(i)['Sad'] for i in t]\n",
    "wsb['Fear']=[dict(i)['Fear'] for i in t]\n",
    "dominant=[]\n",
    "for i in t:\n",
    "    p=dict(i)\n",
    "    Keymax = max(p, key=p.get)\n",
    "    dominant.append(Keymax)\n",
    "wsb['dominant emotion']=dominant\n",
    "day_name= ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday']\n",
    "wsb['timestamp']=pd.to_datetime(wsb['time'])\n",
    "wsb['date']=wsb['timestamp'].dt.day\n",
    "wsb['weekday']=wsb['timestamp'].dt.weekday\n",
    "wsb['weekday']=wsb['weekday'].apply(lambda x: day_name[x])\n",
    "wsb['hour']=wsb['timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f7af4-8168-4988-96a6-90b22c0a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13a7ad-e682-405a-ae7a-a60f51a6f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub= wsb[['time', 'title', 'dominant emotion', 'open', 'high', 'low', 'close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c4f0-0ddc-4030-b4d1-891ddddefff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub.to_csv('wsbgme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975a841-9cf8-4822-a0ae-f7c6d1eb3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://public.tableau.com/app/profile/collins.o8401/viz/NaturalLanguageProcessingusingReddit/Dashboard2#1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
